{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "2d7459c4-c5e5-4372-bd2f-fcc6b0f8c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from keras.layers import TextVectorization,MultiHeadAttention,Dense,Embedding,Dropout,Input,LayerNormalization,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "ef7b8ddf-05e7-41c8-87fc-3c85ac9d4afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_embedding(emb_dim=256, sequence_length=64):\n",
    "    \"\"\"positional embedding for each input token\"\"\"\n",
    "    output = []\n",
    "    for pos in range(sequence_length):\n",
    "        PE = np.zeros(emb_dim, dtype=\"float32\");\n",
    "        for i in range(emb_dim):\n",
    "            if i % 2 == 0:\n",
    "                PE[i] = np.sin(pos / 10000 ** (i / emb_dim))\n",
    "            else:\n",
    "                PE[i] = np.cos(pos / 10000 ** ((i - 1) / emb_dim))\n",
    "        output.append(tf.expand_dims(PE, axis=0))\n",
    "    out = tf.concat(output, axis=0)\n",
    "    out = tf.expand_dims(out, axis=0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "f8b63d4f-7da3-41a7-aec1-6dfc70a2c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, emb_dim, sequence_length):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.token_embeddings = keras.layers.Embedding(input_dim=vocab_size, output_dim=emb_dim)\n",
    "\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        x = self.token_embeddings(inputs)\n",
    "        y = positional_embedding(emb_dim=self.emb_dim, sequence_length=self.sequence_length)\n",
    "        return x + y\n",
    "\n",
    "    def compute_mask1(self, input):\n",
    "        return tf.math.not_equal(input, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"emb_dim\": self.emb_dim,\n",
    "            \"sequence_length\": self.sequence_length\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "2a814441-8ca1-4e77-a092-d4d4f55ab1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(keras.layers.Layer):\n",
    "    def __init__(self, num_heads, dense_dim, emd_dim):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dense_dim = dense_dim\n",
    "        self.emd_dim = emd_dim\n",
    "\n",
    "        self.attention1 = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.emd_dim)\n",
    "        self.layernorm1 = LayerNormalization()\n",
    "        self.layernorm2 = LayerNormalization()\n",
    "        self.linear_projection = keras.models.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(units=self.dense_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(units=self.emd_dim)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = tf.cast(mask[:, newaxis, :], dtype=\"int32\")\n",
    "            T = tf.shape(mask)[2]\n",
    "            mask = tf.repeat(mask, T, axis=1)\n",
    "\n",
    "        attention_output1 = self.attention1(query=inputs, key=inputs, value=inputs, attention_mask=mask)\n",
    "\n",
    "        norm1 = self.layernorm1(attention_output1 + inputs)\n",
    "        linear_proj = self.linear_projection(norm1)\n",
    "        return self.layernorm2(linear_proj + norm1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "            \"emd_dim\": self.emd_dim\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "0690f16e-5645-4875-9225-c75dc70e62a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "EMBEDDING_DIM = 256\n",
    "num_heads = 2\n",
    "num_layers = 1\n",
    "dense_dim = 1024\n",
    "SEQUENCE_LENGTH = 250\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "c357573d-7f80-4e33-b0cc-1985004439c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"G:\\Ajay\\dataset\\IMDB Movie dataset\\IMDB Dataset.csv\")[:10000]\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "f7339ade-b446-43c6-9430-2dae2c28deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(text):\n",
    "    pc = PorterStemmer()\n",
    "    text=text.translate(str.maketrans('', '',string.punctuation))\n",
    "    words = [pc.stem(word.lower()) for word in text.split() if word not in stop_words]    \n",
    "    return \" \".join(words)\n",
    "\n",
    "def change_sentiment(sentiment):\n",
    "    if sentiment == \"positive\":\n",
    "       return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "54e86eca-021d-4789-9adb-bbe32cc18ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"review\"]=data[\"review\"].apply(lambda x:cleanup_text(x))\n",
    "data[\"sentiment\"]=data[\"sentiment\"].apply(lambda x:change_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "eff7725d-c359-4b70-afa6-ec8ea9af62f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one review mention watch 1 oz episod youll hoo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonder littl product br br the film techniqu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought wonder way spend time hot summer wee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basic there famili littl boy jake think there ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visual stun film...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  one review mention watch 1 oz episod youll hoo...          1\n",
       "1  a wonder littl product br br the film techniqu...          1\n",
       "2  i thought wonder way spend time hot summer wee...          1\n",
       "3  basic there famili littl boy jake think there ...          0\n",
       "4  petter mattei love time money visual stun film...          1"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "c9aed9a7-2102-40d2-88f6-9215e270d9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5028\n",
       "0    4972\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "340af344-dc0b-4560-88f6-924abc2a7c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data.iloc[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "5b94ce9e-36a1-4744-b54f-2c5b3de1871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_layer = TextVectorization(\n",
    "    max_tokens = VOCAB_SIZE,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    "    name=\"vectorizer_layer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "e13fc7c2-fc66-421c-bf3e-f3726718c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_layer.adapt(data[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "5b0a3169-6ca3-4883-ac0e-981e7cdf3c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(review):\n",
    "    return vectorizer_layer(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "b4f6dd4b-cfab-4943-8fb3-ca80f5c34e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.apply(lambda x : vectorizer(x[\"review\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "6987dae2-35a1-46fd-83d2-8a6aa4853e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "4d66c032-c765-4064-b15f-ab319773f860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.series.Series, pandas.core.series.Series)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X),type(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "86c94fe7-169c-40ed-8788-4add8ea9ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=0,stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "fa964ac5-90af-40bc-872f-e11a4e1d3703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.series.Series,\n",
       " 5073    (tf.Tensor(2, shape=(), dtype=int64), tf.Tenso...\n",
       " 3656    (tf.Tensor(6296, shape=(), dtype=int64), tf.Te...\n",
       " 2737    (tf.Tensor(10, shape=(), dtype=int64), tf.Tens...\n",
       " 1211    (tf.Tensor(7, shape=(), dtype=int64), tf.Tenso...\n",
       " 3882    (tf.Tensor(2, shape=(), dtype=int64), tf.Tenso...\n",
       "                               ...                        \n",
       " 5557    (tf.Tensor(1, shape=(), dtype=int64), tf.Tenso...\n",
       " 838     (tf.Tensor(902, shape=(), dtype=int64), tf.Ten...\n",
       " 7680    (tf.Tensor(3585, shape=(), dtype=int64), tf.Te...\n",
       " 3114    (tf.Tensor(10, shape=(), dtype=int64), tf.Tens...\n",
       " 6720    (tf.Tensor(9, shape=(), dtype=int64), tf.Tenso...\n",
       " Length: 8000, dtype: object)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train),x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2eb597-fd4a-4630-8db1-1cd404cf7588",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "41697029-bad6-49fc-9a04-50c8bdeb7575",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = keras.layers.Input(shape=(None,))\n",
    "emb = Embeddings(VOCAB_SIZE, EMBEDDING_DIM, sequence_length=SEQUENCE_LENGTH)\n",
    "x = emb(encoder_input)\n",
    "enc_mask = emb.compute_mask1(encoder_input)\n",
    "\n",
    "for i in range(num_layers):\n",
    "    x = TransformerEncoder(num_heads, dense_dim, EMBEDDING_DIM)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "output = keras.layers.Dense(1, activation=\"softmax\")(x)\n",
    "model = keras.models.Model(inputs=encoder_input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "fbc41508-55c1-4901-8d11-ea08ea67b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./sentiment_anaysis_with_transformer.h5\"\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_dir,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "c80d4e4c-3dcb-47d0-8449-f202fbf3d31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (8000,))"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.asarray(x_train)\n",
    "type(x_train),x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "dd9a02df-d223-4e4b-8789-0d19a16ad467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "x_train,y_train = shuffle(x_train,y_train)\n",
    "x_test,y_test = shuffle(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "0a2d4a94-5c1b-4240-98fb-a4491faefcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_x_train = []\n",
    "for i in x_train:\n",
    "    tf_x_train.append(i)\n",
    "tf_x_train=tf.convert_to_tensor(tf_x_train)\n",
    "\n",
    "tf_y_train = tf.convert_to_tensor(y_train.values)\n",
    "tf_y_test  = tf.convert_to_tensor(y_test.values)\n",
    "\n",
    "tf_x_test = []\n",
    "for i in x_test:\n",
    "    tf_x_test.append(i)\n",
    "tf_x_test=tf.convert_to_tensor(tf_x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a2fdf-db94-4e79-9f77-929ad3998926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "250/250 [==============================] - 19s 36ms/step - loss: 0.7392 - accuracy: 0.5027 - val_loss: 0.5798 - val_accuracy: 0.5030\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.5172 - accuracy: 0.5027 - val_loss: 0.5072 - val_accuracy: 0.5030\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4812 - accuracy: 0.5027 - val_loss: 0.5099 - val_accuracy: 0.5030\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.2832 - accuracy: 0.5027 - val_loss: 0.5193 - val_accuracy: 0.5030\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.1876 - accuracy: 0.5027 - val_loss: 0.7153 - val_accuracy: 0.5030\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.1059 - accuracy: 0.5027 - val_loss: 0.8192 - val_accuracy: 0.5030\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.0673 - accuracy: 0.5027 - val_loss: 0.9180 - val_accuracy: 0.5030\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.0565 - accuracy: 0.5027 - val_loss: 1.2594 - val_accuracy: 0.5030\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.0347 - accuracy: 0.5027 - val_loss: 1.4587 - val_accuracy: 0.5030\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.0218 - accuracy: 0.5027 - val_loss: 1.4451 - val_accuracy: 0.5030\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.0356 - accuracy: 0.5027 - val_loss: 1.2328 - val_accuracy: 0.5030\n",
      "Epoch 12/20\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.0540 - accuracy: 0.5027 - val_loss: 1.3667 - val_accuracy: 0.5030\n",
      "Epoch 13/20\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.0267 - accuracy: 0.5027 - val_loss: 1.6773 - val_accuracy: 0.5030\n",
      "Epoch 14/20\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.0142 - accuracy: 0.5027 - val_loss: 1.8769 - val_accuracy: 0.5030\n",
      "Epoch 15/20\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.0217 - accuracy: 0.5027 - val_loss: 1.6211 - val_accuracy: 0.5030\n",
      "Epoch 16/20\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.0109 - accuracy: 0.5027 - val_loss: 1.8474 - val_accuracy: 0.5030\n",
      "Epoch 17/20\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.0181 - accuracy: 0.5027 - val_loss: 2.7443 - val_accuracy: 0.5030\n",
      "Epoch 18/20\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.0238 - accuracy: 0.5027 - val_loss: 1.1559 - val_accuracy: 0.5030\n",
      "Epoch 19/20\n",
      "238/250 [===========================>..] - ETA: 0s - loss: 0.0086 - accuracy: 0.5014"
     ]
    }
   ],
   "source": [
    "adamOpt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=adamOpt, loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics=[\"accuracy\"])\n",
    "history=model.fit(tf_x_train,tf_y_train,validation_data=(tf_x_test,tf_y_test),epochs=20,batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
